{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer assignment 3 - AI\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "<br><br>\n",
    "## Mohammad Amin Baqershahi\n",
    "\n",
    "## 810197464\n",
    "==========================================================================================================================\n",
    "## Introduction\n",
    "In this project we use Naive Bayes Classifier to classify Digikala <br>\n",
    "data which contains a number of comments. In fact comments should <br>\n",
    "be classified. <br>\n",
    "\n",
    "## Project description\n",
    "We have 2 data collection. One for training and one is for testing <br>\n",
    "our model. Each comment in data sets includes a title and a body.<br>\n",
    "Each comment is classified into 2 classes. Recommended and <br>\n",
    "not recommended. In training dataset it is ascertained that each <br>\n",
    "comment belong to which class and this helps us classify comments <br>\n",
    "of test data collection. <br>\n",
    "\n",
    "## Phase 1\n",
    "In this phase we apply a preprocess on both data collections. <br>\n",
    "All possible preprocesses in our code are normalization, removing <br>\n",
    "punctuation, removing stop words, stemming and lemamtization. <br>\n",
    "But in practice applying all of them may cause less accuracy. <br>\n",
    "\n",
    "### Question 1\n",
    "### Stemming and Lemmatization\n",
    "The goal of both stemming and lemmatization is to reduce inflectional <br>\n",
    "forms and sometimes derivationally related forms of a word to a common <br>\n",
    "base form. However they have differences. <br>\n",
    "Stemming looks at the form of word and tries to chop off the ends of it <br>\n",
    "in the hope of achieving the goal. Output word of stemming may be invalid.<br>\n",
    "Stemming is implemented as series of rules that are applied to a word <br>\n",
    "to normalize its form. <br>\n",
    "Lemmatization is more sophisticated than Stemming. It looks at the meaning <br>\n",
    "of the word and use vocabulary analysis. It tries to remove inflectional <br>\n",
    "endings to reach base form of the word. So after applying Lemmatization we <br>\n",
    "will get a valid word. <br>\n",
    "\n",
    "\n",
    "## Phase 2\n",
    "We are to determine for each comment whether is recommended or not and <br>\n",
    "classify them. To do that we must calculate 2 probability for each classes. <br>\n",
    "To calculate them we use general Naive Bayes model formula. In bayes net the <br>\n",
    "root represent classes and it has number of children which are its features. <br>\n",
    "In our project features are repititions of words in each classes. <br> \n",
    "\n",
    "### Question 2\n",
    "* Prior: Before we se an evidence probability of belonging a comment to recommended <br>\n",
    "or not recommended class is the prior in this problem. In fact it does not consider <br>\n",
    "features of comments (words of comments). <br>\n",
    "\n",
    "Calculation: <br>\n",
    "for class recommended: number of recommended comments in training dataset / all comments <br>\n",
    "for class not recommended: number of not recommended comments in training dataset / all comments <br>\n",
    "\n",
    "* Posterior: After we observe evidences that in our problem is the features of comments <br>\n",
    "we know something more in comparison with prior. The probability of belonging a comment <br>\n",
    "to recommended or not recommended class by knowing featues (words of comments) is posterior <br>\n",
    "\n",
    "Calculation: <br>\n",
    "(likehood * prior) / evidence <br>\n",
    "\n",
    "* Evidence: The probability of a particular features (words in comments).\n",
    "\n",
    "Calculation: <br>\n",
    "It calculated by calculating P(word) for all words and multiply all of them. <br>\n",
    "P(word) for each word is: number of all repititions of that word/ all number of words <br>\n",
    "\n",
    "* Likehood: The probability of having a particular features (words in comments) by knowing <br>\n",
    "which class the comment belong.\n",
    "\n",
    "Calculation: <br>\n",
    "It calculated by calculating P(word|class) for all words and multiply all of them. <br>\n",
    "P(word|class) for each word and class is: number of repititions of that word in class / all words in class <br>\n",
    "\n",
    "\n",
    "## Additive smoothing\n",
    "### Question 3\n",
    "Assume a word (w) which its number of repetitions in a class is 0 (in training <br>\n",
    "set). If a comment in test dataset contains this word, in calculatation of P(C|f) <br>\n",
    "for that class term of P(w|C) is 0. So probability for that class become 0. <br>\n",
    "Because the whole term is multiplied by P(w|C). Hence we select the other <br>\n",
    "class as output. This may be not right. <br>\n",
    "\n",
    "### Question 4\n",
    "To avoid the problem explained in previous question, we use additive smoothing. <br>\n",
    "In additive smoothing we include a alpha parameter that solve this problem. Alpha will <br>\n",
    "be added to the numerator and denominator in formula. The alpha prevent it to get 0.<br>\n",
    "The alpha represent how much of the probability we want to reassign to <br>\n",
    "the unseen words. <br>\n",
    "\n",
    "## Phase 2\n",
    "### Evaluation\n",
    "\n",
    "In this phase to evaluate the model we calculate 4 metrics. Accuracy, Precision, <br>\n",
    "Recall and F1.\n",
    "\n",
    "### Question 5\n",
    "Precision talks about how accurate our model is out of those predicted recommended, <br>\n",
    "how many of them are actual recommended. <br>\n",
    "When the costs of False recommended is high, Precision is a good metrics <br>\n",
    "Recall actually calculates how many of the actual recommendeds that our <br>\n",
    "model determine correctly <br><br>\n",
    "Assume a case that we have model that labeled all comments as recommended. <br>\n",
    "In this case Recall is the maxmum number that is possible to get. But our <br>\n",
    "model is not good. But in this case Precision is low. So Precision alone is <br>\n",
    "not enough.By considering both of them our evaluation is acceptable. <br><br>\n",
    "Assume the case that distribution of the problem is extremly uneven. For example <br>\n",
    "number of not recommended comments highly outnumber recommended comments. In fact <br>\n",
    "recommended comments are rare. If our model label all of comments as not recommended <br>\n",
    "the Precision is high. But recall is low. So Precision alone is not enough. <br>\n",
    "   \n",
    "### Question 6\n",
    "F1 Score is harmonic mean of Precision and Recall. Harmonic mean is a type of <br>\n",
    "average generally used for numbers that represent a rate or ratio such as the <br>\n",
    "precision and the recall. <br>\n",
    "By combining precision and recall it consider both false recommendeds <br>\n",
    "and false not recommendeds. In uneven class distributions, F1 is can be more <br>\n",
    "useful than Accuracy.\n",
    "\n",
    "### Question 7\n",
    "| Model | Accuracy | Precision | Recall | F1 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Preprocess and Additive smoothing | 0.9162 | 0.9172 | 0.915 | 0.9161 |\n",
    "| only Additive smoothing | 0.8962 | 0.8711 | 0.93 | 0.8996 |\n",
    "| only Preprocess | 0.6537 | 0.5927 | 0.9825 | 0.7394 |\n",
    "| nothing | 0.6362 | 0.5797 | 0.99 | 0.7313 |\n",
    "\n",
    "### Question 8\n",
    "As we see in the results in preveious question Additive smoothing <br>\n",
    "cause the metrics to increase and improve our model. The reason is <br>\n",
    "it is often to see new words in test data that we do not seen in <br>\n",
    "in train data. If we do not use Additive smoothing some probabilities <br>\n",
    "become 0 and this cause our model to become less accurate. <br>\n",
    "Adding preprocess also can be usefull. In comments some words are <br>\n",
    "repetitive and are not important. In addition punctuations are repetitive. <br>\n",
    "It is good to remove them. They may affect our model. We can see that <br>\n",
    "results in preveious question become better by applying preprocess. <br>\n",
    "We can see that gradient of Recall is different than other metrics. <br>\n",
    "In the case that we do not use Preprocess and Additive smoothing <br>\n",
    "lots of comments labeled as recommended. Some of them wrongly labeled. <br>\n",
    "This cause recall to be a high number. As the model become more accurate <br>\n",
    "by Preprocess and Additive smoothing value of Recall decrease and other <br>\n",
    "metrics increase. <br>\n",
    "\n",
    "### Question 9\n",
    "\n",
    "* Comment 1: با این قیمت گزینه های بهتری هم میشه گرفت.\n",
    "روان مینویسه ولی زیاد مناسب نیست و رنگ پس میده یه وقتایی موقع نوشتن\n",
    "\n",
    "همانطور که دیده می شود در این مثال تعدادی صفت مثبت وجود دارد. مانند روان و مناسب و بهتر. اما افعال منفی با این صفت ها به کار رفته اند. در مرحله پیش پردازش چنین افعالی حذف شده اند یا با ریشه خود جایگزین شده اند. این باعث اشتباه تشخیص دادن مدل ما شده است. و آن را مثبت تشخیص داده است.\n",
    "\n",
    "* Comment 2: محصول بی کیفیت . زمان تماس که اصلا صدای واضحی نداره و کلا صدا نمیره ب طرف مقابل . وقتی اولین بار گذاشتم تو گوشم و درآوردمش کلا جدا شد قسمت پلاستیکی و سیمش مشخص شد . ینی اصلا ب درد نمیخوره\n",
    "\n",
    "در این مثال نیز مشابه مثال قبل مشاهده می کنیم که از کلماتی مانند بی کیفیت و کیفیت استفاده شده است.  پیشوند های آن ها در مرحله نرمالایز کردن حذف شده است و این باعث شده آن را مثبت تشخیص دهد.\n",
    "\n",
    "* Comment 3: من این ادو تویلتو خریدم \n",
    "بر خلاف اینکه نوشته خنک  خیلی هم خنک نیست ضمنا بوی شیرینی و کاراملش خیلی غالبه . انتظار داشتم بوی گلش غالب باشه\n",
    "ولی در کل با کیفیت و زیباست. مانگاری و پخش بوی خیلی خوبی داره .\n",
    "\n",
    "در این مثال کلمات خاصی مانند خلاف وجود دارد که منفی است و باعث گمراهی مدل ما شده است. و آن را منفی تشخیص داده است.\n",
    "\n",
    "* Comment 4: اگر خریداری نکنید بهتره. انقدر که در تصویر دیده می شود زیبا نیست\n",
    "\n",
    "در این مورد نیز مشابه موارد قبل کلمه زیبا باعث شده مدل آن را مثبت تشخیص دهد.\n",
    "\n",
    "* Comment 5: مشکل فقط اندازه اش بود یه کم گشاد بودوگرنه جنس خوب داره\n",
    "\n",
    "کلماتی مانند مشکل و گشاد در این کامنت وجود دارد که باعث شده به اشتباه مدل آن را منفی تشخیص دهد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Accuracy:  0.91625\n",
      "Precision:  0.9172932330827067\n",
      "Recall:  0.915\n",
      "F1:  0.916145181476846\n",
      "-----------\n",
      "The answer:\n",
      "['recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'recommended', 'recommended', 'recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'not_recommended', 'recommended', 'recommended', 'not_recommended']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from hazm import stopwords_list\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, comment_train_path, comment_test_path):\n",
    "        self.comment_train = pd.read_csv(comment_train_path)\n",
    "        self.comment_test = pd.read_csv(comment_test_path)\n",
    "        \n",
    "        self.comment_train = self.comment_train.values.tolist()\n",
    "        self.comment_test = self.comment_test.values.tolist()\n",
    "        \n",
    "        self.stop_words = set(stopwords_list())\n",
    "        self.punctuations = '''!()-[]{};:'\"\\,؟ <>./?@#$%^&*_~'''\n",
    "        \n",
    "        \n",
    "    def remove_stop_words(self, words):\n",
    "        output = []\n",
    "        for word in words: \n",
    "            if word not in self.stop_words:\n",
    "                output.append(word)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def remove_punctuations(self, comment):\n",
    "        for c in comment:  \n",
    "            if c in self.punctuations:  \n",
    "                comment = comment.replace(c, \" \")\n",
    "        \n",
    "        return comment\n",
    "    \n",
    "    \n",
    "    def pre_process(self, comments, en_normalize, en_stem, en_lemmatize, en_remove_stop_words, en_remove_punctuations):\n",
    "        normalizer = Normalizer()\n",
    "        lemmatizer = Lemmatizer()\n",
    "        stemmer = Stemmer()\n",
    "\n",
    "        for i in range(len(comments)):\n",
    "            for j in range(2):\n",
    "                # remove punctuations\n",
    "                if en_remove_punctuations:\n",
    "                    comments[i][j] = self.remove_punctuations(comments[i][j])\n",
    "                \n",
    "                # Normalize\n",
    "                if en_normalize:\n",
    "                    comments[i][j] = normalizer.normalize(comments[i][j])\n",
    "                \n",
    "                comments[i][j] = word_tokenize(comments[i][j])\n",
    "                \n",
    "                # Remove stop words\n",
    "                if en_remove_stop_words:\n",
    "                     comments[i][j] = self.remove_stop_words(comments[i][j])\n",
    "                \n",
    "                # Stem\n",
    "                if en_stem:\n",
    "                    for k in range(len(comments[i][j])):\n",
    "                        comments[i][j][k] = stemmer.stem(comments[i][j][k])\n",
    "                \n",
    "                # Lemmatize\n",
    "                if en_lemmatize:\n",
    "                    for k in range(len(comments[i][j])):\n",
    "                        comments[i][j][k] = lemmatizer.lemmatize(comments[i][j][k])\n",
    "                        \n",
    "        return comments\n",
    "                \n",
    "\n",
    "    def calculate_word_repetitions(self):\n",
    "        self.recommended_number = 0\n",
    "        self.not_recommended_number = 0\n",
    "        \n",
    "        self.recommended_repititions = {}\n",
    "        self.not_recommended_repititions = {}\n",
    "        \n",
    "        for i in range(len(self.comment_train)):\n",
    "            for j in range(2):\n",
    "                for k in range(len(self.comment_train[i][j])):\n",
    "                    if self.comment_train[i][2] == \"recommended\":\n",
    "                        self.recommended_number += 1\n",
    "                        if self.comment_train[i][j][k] in self.recommended_repititions:\n",
    "                            self.recommended_repititions[self.comment_train[i][j][k]] += 1\n",
    "                        else:\n",
    "                            self.recommended_repititions[self.comment_train[i][j][k]] = 0\n",
    "    \n",
    "                    elif self.comment_train[i][2] == \"not_recommended\":\n",
    "                        self.not_recommended_number += 1\n",
    "                        if self.comment_train[i][j][k] in self.not_recommended_repititions:\n",
    "                            self.not_recommended_repititions[self.comment_train[i][j][k]] += 1\n",
    "                        else:\n",
    "                            self.not_recommended_repititions[self.comment_train[i][j][k]] = 0\n",
    "    \n",
    "    \n",
    "    def calculate_probabilities(self, comment, alpha):\n",
    "        p_c_recommended = 1\n",
    "        p_c_not_recommended = 1\n",
    "        \n",
    "        for j in range(2):\n",
    "            for k in range(len(comment[j])):\n",
    "                if comment[j][k] in self.recommended_repititions:\n",
    "                    p_f_recommended = (self.recommended_repititions[comment[j][k]] + alpha) /\\\n",
    "                                (self.recommended_number + len(self.recommended_repititions) * alpha)\n",
    "                else:\n",
    "                    p_f_recommended = alpha / (self.recommended_number + len(self.recommended_repititions) * alpha)\n",
    "                \n",
    "                p_c_recommended *= p_f_recommended\n",
    "                \n",
    "                if comment[j][k] in self.not_recommended_repititions:\n",
    "                    p_f_not_recommended = (self.not_recommended_repititions[comment[j][k]] + alpha) /\\\n",
    "                                (self.not_recommended_number + len(self.not_recommended_repititions) * alpha)\n",
    "                else:\n",
    "                    p_f_not_recommended = alpha / (self.not_recommended_number + len(self.not_recommended_repititions) * alpha)\n",
    "                    \n",
    "                p_c_not_recommended *= p_f_not_recommended\n",
    "        \n",
    "        p_c_recommended *= self.p_recommended\n",
    "        p_c_not_recommended *= self.p_not_recommended\n",
    "        \n",
    "        return p_c_recommended, p_c_not_recommended\n",
    "                \n",
    "    \n",
    "    def calculate_answers(self, alpha):\n",
    "        self.p_recommended =  self.recommended_number / (self.recommended_number + self.not_recommended_number)\n",
    "        self.p_not_recommended = 1 - self.p_recommended\n",
    "\n",
    "        correct_detected_recommended = 0\n",
    "        correct_detected_not_recommended = 0\n",
    "        all_detected_recommended = 0\n",
    "        total_recommended = 0\n",
    "        total_not_recommended = 0\n",
    "        \n",
    "        self.answers = []\n",
    "        for i in range(len(self.comment_test)):\n",
    "            if self.comment_test[i][2] == \"recommended\":\n",
    "                total_recommended += 1\n",
    "            else:\n",
    "                total_not_recommended += 1\n",
    "                \n",
    "            p_c_recommended, p_c_not_recommended = self.calculate_probabilities(self.comment_test[i], alpha)\n",
    "            if p_c_recommended >= p_c_not_recommended:\n",
    "                self.answers.append(\"recommended\")\n",
    "                all_detected_recommended += 1\n",
    "                if self.comment_test[i][2] == \"recommended\":\n",
    "                    correct_detected_recommended += 1\n",
    "            else:\n",
    "                self.answers.append(\"not_recommended\")\n",
    "                if self.comment_test[i][2] == \"not_recommended\":\n",
    "                    correct_detected_not_recommended += 1\n",
    "        \n",
    "        self.accuracy = (correct_detected_recommended + correct_detected_not_recommended) / len(self.comment_test)\n",
    "        self.precision = correct_detected_recommended / all_detected_recommended\n",
    "        self.recall = correct_detected_recommended / total_recommended\n",
    "        self.f1 = 2 * ((self.precision * self.recall) / (self.precision + self.recall))\n",
    "        \n",
    "        \n",
    "    def algorithm(self):\n",
    "        self.comment_train = self.pre_process(self.comment_train, True, False, False, True, True)\n",
    "        self.comment_test = self.pre_process(self.comment_test, True, False, False, True, True)\n",
    "        self.calculate_word_repetitions()\n",
    "        self.calculate_answers(1);        \n",
    "        \n",
    "        print(\"-----------\")\n",
    "        print(\"Accuracy: \", self.accuracy)\n",
    "        print(\"Precision: \", self.precision)\n",
    "        print(\"Recall: \", self.recall)\n",
    "        print(\"F1: \", self.f1)\n",
    "        print(\"-----------\")\n",
    "        print(\"The answer:\")\n",
    "        print(self.answers)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "naive_bayes_classifier = NaiveBayesClassifier(\"comment_train.csv\", \"comment_test.csv\")\n",
    "naive_bayes_classifier.algorithm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
